{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Algorithm's analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations, groupby\n",
    "\n",
    "from networkx.algorithms import tree\n",
    "from networkx.algorithms import floyd_warshall_predecessor_and_distance\n",
    "\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can use this function to generate a random graph with 'num_of_nodes' nodes\n",
    "# and 'completeness' probability of an edge between any two nodes\n",
    "# If 'directed' is True, the graph will be directed\n",
    "# If 'draw' is True, the graph will be drawn\n",
    "def gnp_random_connected_graph(num_of_nodes: int,\n",
    "                               completeness: int,\n",
    "                               directed: bool = False,\n",
    "                               draw: bool = False):\n",
    "    \"\"\"\n",
    "    Generates a random graph, similarly to an Erdős-Rényi \n",
    "    graph, but enforcing that the resulting graph is conneted (in case of undirected graphs)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if directed:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G = nx.Graph()\n",
    "    edges = combinations(range(num_of_nodes), 2)\n",
    "    G.add_nodes_from(range(num_of_nodes))\n",
    "    \n",
    "    for _, node_edges in groupby(edges, key = lambda x: x[0]):\n",
    "        node_edges = list(node_edges)\n",
    "        random_edge = random.choice(node_edges)\n",
    "        if random.random() < 0.5:\n",
    "            random_edge = random_edge[::-1]\n",
    "        G.add_edge(*random_edge)\n",
    "        for e in node_edges:\n",
    "            if random.random() < completeness:\n",
    "                G.add_edge(*e)\n",
    "                \n",
    "    for (u,v,w) in G.edges(data=True):\n",
    "        w['weight'] = random.randint(-5, 20)\n",
    "                \n",
    "    if draw: \n",
    "        plt.figure(figsize=(10,6))\n",
    "        if directed:\n",
    "            # draw with edge weights\n",
    "            pos = nx.arf_layout(G)\n",
    "            nx.draw(G,pos, node_color='lightblue', \n",
    "                    with_labels=True,\n",
    "                    node_size=500, \n",
    "                    arrowsize=20, \n",
    "                    arrows=True)\n",
    "            labels = nx.get_edge_attributes(G,'weight')\n",
    "            nx.draw_networkx_edge_labels(G, pos,edge_labels=labels)\n",
    "            \n",
    "        else:\n",
    "            nx.draw(G, node_color='lightblue', \n",
    "                with_labels=True, \n",
    "                node_size=500)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function we made to compare the execution times of different algorithms.\n",
    "\n",
    "import timeit\n",
    "\n",
    "def testing_algorithms(sizes, edge_probs, builtin_algorithm, custom_algorithm, number, directed=False):\n",
    "    \"\"\"\n",
    "    Compare and visualize different algorithms\n",
    "\n",
    "    :param sizes: list, A list of sizes of graphs on which we should test the algorithms\n",
    "    :param edge_probs: float, The probability for extra edges to occur in every testing graph\n",
    "    :param builtin_algorithm: callable, The built-in algorithm\n",
    "    :param custom_algorithm: callable, The algorithm to compare the built-in to\n",
    "    :param number: int, The number of times to run every algoritm\n",
    "    \"\"\"\n",
    "    builtin_times = []\n",
    "    custom_times = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        G = gnp_random_connected_graph(size, edge_probs, directed)\n",
    "        \n",
    "        builtin_time = timeit.timeit(lambda: builtin_algorithm(G), number=number)\n",
    "        custom_time = timeit.timeit(lambda: custom_algorithm(G), number=number)\n",
    "        \n",
    "        builtin_times.append(builtin_time)\n",
    "        custom_times.append(custom_time)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sizes, builtin_times, label='Built-in algorithm', marker='x')\n",
    "    plt.plot(sizes, custom_times, label='Our algorothm', marker='s')\n",
    "\n",
    "    plt.xlabel('Graph Size (Number of Nodes)')\n",
    "    plt.ylabel('Execution Time (Seconds)')\n",
    "    plt.title('Performance Comparison of Algorithms')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1.1: Prim's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gnp_random_connected_graph(10, 0.5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstp = tree.minimum_spanning_tree(G, algorithm=\"prim\")\n",
    "\n",
    "nx.draw(mstp, node_color='lightblue', \n",
    "        with_labels=True, \n",
    "        node_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our implementation\n",
    "Prim’s algorithm finds the Minimum Spanning Tree of a graph, meaning it connects all the nodes with the smallest total edge weight without forming a cycle.\n",
    "\n",
    "How it works:\n",
    "\n",
    "* Start with any node in the graph.\n",
    "* Pick the smallest edge that connects the current tree to a new node.\n",
    "* Add that node to the tree and mark it as visited.\n",
    "* Repeat until all nodes are connected.\n",
    "\n",
    "It keeps adding the cheapest possible edge that expands the tree while avoiding cycles. The result is a minimum-cost tree connecting all nodes.\n",
    "\n",
    "### Time Complexity:\n",
    "The algorithm runs in O(V²) time, where V is the number of nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def prim(original_graph: nx.Graph, starting_node: int = 0):\n",
    "    mst = nx.Graph()\n",
    "    mst.add_nodes_from(original_graph)\n",
    "    min_heap = []\n",
    "\n",
    "    def explore_node(node: int):\n",
    "        mst.nodes[node]['visited'] = True\n",
    "        for neighbor in original_graph.neighbors(node):\n",
    "            if not mst.nodes[neighbor].get('visited', False):\n",
    "                heapq.heappush(min_heap, (original_graph[node][neighbor]['weight'], node, neighbor))\n",
    "\n",
    "    explore_node(starting_node)\n",
    "\n",
    "    while min_heap:\n",
    "        weight, node1, node2 = heapq.heappop(min_heap)\n",
    "\n",
    "        if not mst.nodes[node2].get('visited', False):\n",
    "            mst.add_edge(node1, node2, weight=weight)\n",
    "            explore_node(node2)\n",
    "\n",
    "    return mst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst = prim(G)\n",
    "\n",
    "nx.draw(mst, node_color='lightblue', \n",
    "        with_labels=True, \n",
    "        node_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing ours vs built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we specify the graph sizes on which we should test the algorithms.\n",
    "sizes = [10, 25, 50, 75, 100]\n",
    "edge_prob = 0.05\n",
    "\n",
    "# The variable below is used to specify the number of how many times must each algorithm execute.\n",
    "# Feel free to modify it if you feel like the testing takes too long.\n",
    "number = 1000\n",
    "\n",
    "# Running the testing\n",
    "testing_algorithms(sizes, edge_prob, lambda G: tree.minimum_spanning_tree(G, algorithm=\"prim\"), prim, number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excecution Times:\n",
    "As expected, the built-in algorithm operates faster, although, interestingly enough, the lesser edge count gets, the faster operates our algorithm, even outracing the built-in one on values of edge_prob lesser than 0.05\n",
    "\n",
    "#### Conclusion:\n",
    "* Built-in algorithm is faster, obviously. I'm no such Python god."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1.2: Floyd-Warshall algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gnp_random_connected_graph(10, 0.5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred is a dictionary of predecessors, dist is a dictionary of distances dictionaries\n",
    "try:\n",
    "    pred, dist = floyd_warshall_predecessor_and_distance(G) \n",
    "    for k, v in dist.items():\n",
    "        print(f\"Distances with {k} source:\", dict(v))\n",
    "except:\n",
    "    print(\"Negative cycle detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our implementation\n",
    "The Floyd-Warshall algorithm finds the shortest paths between all pairs of nodes in a graph. \n",
    "It works by improving distance estimates using every node as a possible stop between two other nodes.\n",
    "\n",
    "1. The algorithm stars by creating a distance dictionary where:\n",
    "* The distance from a node to itself is 0.\n",
    "* The distance between two directly connected nodes is set to the edge weight.\n",
    "* The distance between all other nodes is set to infinity.\n",
    "2. For each node, it checks if going through it makes the path between two other nodes shorter. If it is true, it updates the distance.\n",
    "3. The algorithm repeats this process until all possible shorter paths are found.\n",
    "4. the function also checks for a negative cycle, which is when the sum of weights in a loop is negative, leading to infinitely decreasing distances.\n",
    "If `distance[node][node] < 0` for any node, the graph has a negative cycle.\n",
    "\n",
    "### Time Complexity:\n",
    "The algorithm runs in O(V³) time, where V is the number of nodes in the graph. This is because it checks all possible pairs of nodes for every intermediate node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floyd_warshall(G: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Calculates the shortest path distances between all pairs of nodes in the graph \n",
    "    using the Floyd-Warshall algorithm and returns a new graph with the shortest path \n",
    "    distances as edge weights\n",
    "    args:\n",
    "        original_graph (nx.Graph): an undirected graph represented as a NetworkX Graph\n",
    "    returns:\n",
    "        nx.Graph: a new graph where edges represent the shortest path distances between nodes\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes)\n",
    "    distance = {}\n",
    "    for u in G.nodes:\n",
    "        distance[u] = {}\n",
    "        for v in G.nodes:\n",
    "            distance[u][v] = float('inf')\n",
    "    for u in nodes:\n",
    "        distance[u][u] = 0\n",
    "\n",
    "    edges = list(G.edges(data=True))\n",
    "    for u, v, data in edges:\n",
    "        weight = data.get('weight', 1)\n",
    "        distance[u][v] = weight\n",
    "\n",
    "    for k in nodes:\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                if distance[i][k] != float('inf') and distance[k][j] != float('inf'):\n",
    "                    if distance[i][j] > distance[i][k] + distance[k][j]:\n",
    "                        distance[i][j] = distance[i][k] + distance[k][j]\n",
    "\n",
    "    for node in nodes:\n",
    "        if distance[node][node] < 0:\n",
    "            pass\n",
    "\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    distance = floyd_warshall(G)\n",
    "    for source, targets in distance.items():\n",
    "        print(f\"Distances with {source} source:\", targets)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing ours vs built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we specify the graph sizes on which we should test the algorithms.\n",
    "sizes = [10, 25, 50, 75, 100]\n",
    "edge_prob = 0.5\n",
    "# The variable below is used to specify the number of how many times must each algorithm execute.\n",
    "# Feel free to modify it if you feel like the testing takes too long.\n",
    "number = 100\n",
    "\n",
    "# Running the testing\n",
    "testing_algorithms(sizes, edge_prob, nx.floyd_warshall_predecessor_and_distance, floyd_warshall, number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution Times:\n",
    "For smaller graph sizes like 10 vertices, both our implementation and the built-in Floyd-Warshall have very similar execution times, with a difference of less than 1 second.\n",
    "\n",
    "#### As the graph size increases:\n",
    " \n",
    "* For 50 vertices, the time difference becomes more noticeable, with a bit more than 1 second difference\n",
    "* For 100 vertices and more, the gap is much larger. For example, our implementation is taking over 20 seconds on 100 vertices, while the built-in algorithm takes less than 10 seconds, which is a big gap. \n",
    "\n",
    "#### Analysis of Differences:\n",
    "At first the times are similar with a second difference less than a second.\n",
    "For large graphs, however, the NetworkX function has to perform extra tasks like finding paths, it's designed to be more efficient and handles large graphs better.\n",
    "Python can be slow when working with large amounts of data, which also slows down our implementation.\n",
    "\n",
    "\n",
    "#### Where the biggest difference occurs:\n",
    "The difference is most significant for larger graphs with 75 and 100 vertices. This is because our implementation's nested loops that slow down the proces with the increasing number of vertices. The built-in algorithm is more optimized for such larger graphs.\n",
    "\n",
    "#### Conclusion:\n",
    "* For small graphs, both algorithms are about the same.\n",
    "* For larger graphs, the difference in execution time grows significantly. It is mainly because of the Python's slower execution of loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "Our goal in this project was to implement a decision tree classifier in Python to classify Iris flowers into three species: Iris Setosa, Iris Versicolor, and Iris Virginica. By using the Iris dataset, we aimed to understand how decision trees work in machine learning. \n",
    "The goal of the decision tree that we created is to split data into subsets based on the given values to make predictions on to which species each flower belongs. At each node of the tree, a decision is made to split the data into two groups, and this continues recursively until the tree is built.\n",
    "\n",
    "### Dataset information:\n",
    "The Iris dataset consists of 150 samples. Each sample belongs to one of three species. Each sample includes the following four features, measured in centimeters:\n",
    "\n",
    "* Sepal Length\n",
    "* Sepal Width\n",
    "* Petal Length\n",
    "* Petal Width\n",
    "These features are an input for our decision tree model to distinguish between the three species.\n",
    "\n",
    "### The process:\n",
    "At each node, the algorithm decides which feature and what value to use to split the dataset into left and right. This decision is based on a measure of how  mixed the groups are.\n",
    "\n",
    "The goal when splitting data is to create the purest possible subsets, meaning that the samples in each subset should mostly belong to the same species.\n",
    "We used Gini impurity to measure the purity of the data at each node.\n",
    "\n",
    "The tree-building process is recursive. At each step, the tree chooses the best feature to split the data and a value to divide the data into two groups. After splitting, the process repeats for each of the new groups. This continues until we reach a stopping point, like when the tree reaches its maximum depth or when the data can't be split any further.\n",
    "\n",
    "The goal is to predict the species of a new Iris flower. When we get new data (like a new flower's measurements), the tree looks at the feature values and follows the path from the starting point (root) to the end (leaf) of the tree. At each step, it checks the feature values and moves left or right based on the value. When it reaches the end (a leaf node), it makes a prediction based on the most common species in that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split\n",
    "\n",
    "We train our model using training dataset and evaluate its performance basing on the test dataset. Reason to use two separate datasets is that our model learns its parameters from data, thus test set allows us to check its possibilities on completely new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(X, y, test_size= 0.20)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node class\n",
    "\n",
    "This is the class for nodes, and the whole tree is built from them. It contains methods for calculating the Gini impurity of a node and the heart of the whole algorithm - find_best_split() function. The tree is built in result of multiple executions of this function, which calculates the best condition for splitting a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A class for nodes which together build up a decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: npt.NDArray, y: npt.NDArray, parent = None):\n",
    "        \"\"\"\n",
    "        :param X: numpy array of form [[feature1,feature2, ... featureN], ...] (i.e. [[1.5, 5.4, 3.2, 9.8] , ...] for case with iris d.s.)\n",
    "        :param y: numpy array of from [class1, class2, ...] (i.e. [0,1,1,2,1,0,...] for case with iris d.s.)\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.predicted_classes, self.class_occurences = np.unique(y, return_counts=True)\n",
    "        self.samples_count = len(y)\n",
    "        self.gini = self.calculate_gini()\n",
    "        self.parent = parent\n",
    "\n",
    "    def calculate_gini(self):\n",
    "        \"\"\"\n",
    "        The function for calculating the gini index of a node.\n",
    "        \"\"\"\n",
    "        return 1 - sum((num / self.samples_count)**2 for num in self.class_occurences)\n",
    "\n",
    "    def find_best_split(self):\n",
    "        \"\"\"\n",
    "        The main function in the whole decision tree, ironically not belonging to its class. \n",
    "        Does everything: finds the best feature index and threshold for the node and returns the node's\n",
    "        children. You can even say the build_tree() function is a mere wrapper for this one.\n",
    "        \"\"\"\n",
    "        samples = list(zip(self.X, self.y))\n",
    "        best_info_gain = -float(\"inf\")\n",
    "\n",
    "        for feature_index in range(self.X.shape[1]):\n",
    "            possible_thresholds = np.unique(self.X[:, feature_index])\n",
    "\n",
    "            for threshold in possible_thresholds:\n",
    "                left_samples = []\n",
    "                right_samples = []\n",
    "\n",
    "                for sample in samples:\n",
    "                    if sample[0][feature_index] <= threshold:\n",
    "                        left_samples.append(sample)\n",
    "                    else:\n",
    "                        right_samples.append(sample)\n",
    "\n",
    "                if not len(left_samples) or not len(right_samples):\n",
    "                    continue\n",
    "\n",
    "                left = Node(*map(np.array, zip(*left_samples)), self)\n",
    "                right = Node(*map(np.array, zip(*right_samples)), self)\n",
    "\n",
    "                information_gain = (self.gini - left.samples_count / self.samples_count * left.gini - \n",
    "                    right.samples_count / self.samples_count * right.gini)\n",
    "\n",
    "                if information_gain > best_info_gain:\n",
    "                    if self.parent and threshold == self.parent.left.threshold and feature_index == self.parent.left.feature_index:\n",
    "                        continue\n",
    "                    best_info_gain = information_gain\n",
    "                    best_threshold = threshold\n",
    "                    best_feature_index = feature_index\n",
    "                    best_left = left\n",
    "                    best_right = right\n",
    "        \n",
    "        self.feature_index = best_feature_index\n",
    "        self.threshold = best_threshold\n",
    "        self.left = best_left\n",
    "        self.right = best_right\n",
    "\n",
    "        return best_left, best_right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main class\n",
    "\n",
    "This is the DecisionTreeClassifier class - the main class of the algorithm. It contains methods for building a decision tree out of any suitable dataset and a function for predicting classes for test samples, traversing a trained tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    The main class of a decision tree classifier. Builds a decision tree and stores it as a single root node with\n",
    "    its children as its instance variables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth: int) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def fit(self, X: npt.NDArray, y: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Basically, function that performs all the training (building of a tree)\n",
    "        We recommend to use it as a wrapper of recursive building function\n",
    "        \"\"\"\n",
    "        root_node = Node(X, y)\n",
    "        self.tree = root_node\n",
    "\n",
    "        self.build_tree(root_node)\n",
    "\n",
    "    def build_tree(self, node: Node, depth = 0) -> None:\n",
    "        \"\"\"\n",
    "        Recursive building function\n",
    "        \"\"\"\n",
    "        if depth < self.max_depth:\n",
    "            left, right = node.find_best_split()\n",
    "\n",
    "            if left and left.samples_count >= 2 and left.gini != 0:\n",
    "                self.build_tree(left, depth + 1)\n",
    "\n",
    "            if right and right.samples_count >= 2 and right.gini != 0:\n",
    "                self.build_tree(right, depth + 1)\n",
    "    \n",
    "    def predict(self, X_test: npt.NDArray) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Traverse the tree while there is a child\n",
    "        and return the predicted class for it \n",
    "        \"\"\"\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Cannot predict while the tree is not yet built. Use .fit()\")\n",
    "\n",
    "        predicted_classes = np.empty(X_test.shape[0], dtype=int)\n",
    "\n",
    "        for index, sample in enumerate(list(X_test)):\n",
    "            current_node = self.tree\n",
    "            while current_node.threshold is not None:\n",
    "                if sample[current_node.feature_index] <= current_node.threshold:\n",
    "                    current_node = current_node.left\n",
    "                else:\n",
    "                    current_node = current_node.right\n",
    "\n",
    "            predicted_classes[index] = max(zip(current_node.predicted_classes, current_node.class_occurences), key=lambda x: x[1])[0]\n",
    "\n",
    "        return predicted_classes\n",
    "\n",
    "    def evaluate(self, X_test: list[list], y_test: list) -> float:\n",
    "        \"\"\"\n",
    "        Returns accuracy of the model (ratio of right guesses to the number of samples)\n",
    "        \"\"\"\n",
    "\n",
    "        return float(np.sum(self.predict(X_test) == y_test) / y_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(10)\n",
    "dtc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the accuracy of the model\n",
    "\n",
    "Now we can use our model to predict which type has a flower, basing on its parameters.\n",
    "\n",
    "This is conducted basically via traversing the tree that you can see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a testing dataset\n",
    "predictions = dtc.predict(X_test)\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the accuracy\n",
    "dtc.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we successfully built a decision tree classifier to predict the species of Iris flowers based on their features like sepal and petal length and width. By using the Iris dataset, we trained the model to split the data at each step based on the best features and values. Our aim was to create the most accurate predictions about the class each flower belongs to. The decision tree algorithm recursively built the tree, making predictions by following paths from the root to the leaves. With the performance of the algorithm we were able to classify new samples correctly, which showed us how decision trees can effectively classify data based on feature values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
