{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install graphviz\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations, groupby\n",
    "\n",
    "from networkx.algorithms import tree\n",
    "from networkx.algorithms import bellman_ford_predecessor_and_distance\n",
    "from networkx.algorithms import floyd_warshall_predecessor_and_distance\n",
    "\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Algorithm's analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can use this function to generate a random graph with 'num_of_nodes' nodes\n",
    "# and 'completeness' probability of an edge between any two nodes\n",
    "# If 'directed' is True, the graph will be directed\n",
    "# If 'draw' is True, the graph will be drawn\n",
    "def gnp_random_connected_graph(num_of_nodes: int,\n",
    "                               completeness: int,\n",
    "                               directed: bool = False,\n",
    "                               draw: bool = False):\n",
    "    \"\"\"\n",
    "    Generates a random graph, similarly to an Erdős-Rényi \n",
    "    graph, but enforcing that the resulting graph is conneted (in case of undirected graphs)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if directed:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G = nx.Graph()\n",
    "    edges = combinations(range(num_of_nodes), 2)\n",
    "    G.add_nodes_from(range(num_of_nodes))\n",
    "    \n",
    "    for _, node_edges in groupby(edges, key = lambda x: x[0]):\n",
    "        node_edges = list(node_edges)\n",
    "        random_edge = random.choice(node_edges)\n",
    "        if random.random() < 0.5:\n",
    "            random_edge = random_edge[::-1]\n",
    "        G.add_edge(*random_edge)\n",
    "        for e in node_edges:\n",
    "            if random.random() < completeness:\n",
    "                G.add_edge(*e)\n",
    "                \n",
    "    for (u,v,w) in G.edges(data=True):\n",
    "        w['weight'] = random.randint(-5, 20)\n",
    "                \n",
    "    if draw: \n",
    "        plt.figure(figsize=(10,6))\n",
    "        if directed:\n",
    "            # draw with edge weights\n",
    "            pos = nx.arf_layout(G)\n",
    "            nx.draw(G,pos, node_color='lightblue', \n",
    "                    with_labels=True,\n",
    "                    node_size=500, \n",
    "                    arrowsize=20, \n",
    "                    arrows=True)\n",
    "            labels = nx.get_edge_attributes(G,'weight')\n",
    "            nx.draw_networkx_edge_labels(G, pos,edge_labels=labels)\n",
    "            \n",
    "        else:\n",
    "            nx.draw(G, node_color='lightblue', \n",
    "                with_labels=True, \n",
    "                node_size=500)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function we made to compare the execution times of different algorithms.\n",
    "\n",
    "import timeit\n",
    "\n",
    "def testing_algorithms(sizes, edge_probs, builtin_algorithm, custom_algorithm, allow_negative=False):\n",
    "    builtin_times = []\n",
    "    custom_times = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        G = gnp_random_connected_graph(size, edge_probs, allow_negative)\n",
    "        \n",
    "        builtin_time = timeit.timeit(lambda: builtin_algorithm(G), number=100)\n",
    "        custom_time = timeit.timeit(lambda: custom_algorithm(G), number=100)\n",
    "        \n",
    "        builtin_times.append(builtin_time)\n",
    "        custom_times.append(custom_time)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sizes, builtin_times, label='Built-in algorithm', marker='x')\n",
    "    plt.plot(sizes, custom_times, label='Our algorothm', marker='s')\n",
    "\n",
    "    plt.xlabel('Graph Size (Number of Nodes)')\n",
    "    plt.ylabel('Execution Time (Seconds)')\n",
    "    plt.title('Performance Comparison of Floyd-Warshall Algorithms')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1.2: Floyd-Warshall algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gnp_random_connected_graph(10, 0.5, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred is a dictionary of predecessors, dist is a dictionary of distances dictionaries\n",
    "try:\n",
    "    pred, dist = floyd_warshall_predecessor_and_distance(G) \n",
    "    for k, v in dist.items():\n",
    "        print(f\"Distances with {k} source:\", dict(v))\n",
    "except:\n",
    "    print(\"Negative cycle detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our implementation\n",
    "The Floyd-Warshall algorithm finds the shortest paths between all pairs of nodes in a graph. \n",
    "It works by improving distance estimates using every node as a possible stop between two other nodes.\n",
    "\n",
    "1. The algorithm stars by creating a distance dictionary where:\n",
    "* The distance from a node to itself is 0.\n",
    "* The distance between two directly connected nodes is set to the edge weight.\n",
    "* The distance between all other nodes is set to infinity.\n",
    "2. For each node, it checks if going through it makes the path between two other nodes shorter. If it is true, it updates the distance.\n",
    "3. The algorithm repeats this process until all possible shorter paths are found.\n",
    "4. the function also checks for a negative cycle, which is when the sum of weights in a loop is negative, leading to infinitely decreasing distances.\n",
    "If `distance[node][node] < 0` for any node, the graph has a negative cycle.\n",
    "\n",
    "### Time Complexity:\n",
    "The algorithm runs in O(V³) time, where V is the number of nodes in the graph. This is because it checks all possible pairs of nodes for every intermediate node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floyd_warshall(G: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Calculates the shortest path distances between all pairs of nodes in the graph \n",
    "    using the Floyd-Warshall algorithm and returns a new graph with the shortest path \n",
    "    distances as edge weights\n",
    "    args:\n",
    "        original_graph (nx.Graph): an undirected graph represented as a NetworkX Graph\n",
    "    returns:\n",
    "        nx.Graph: a new graph where edges represent the shortest path distances between nodes\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes)\n",
    "    distance = {}\n",
    "    for u in G.nodes:\n",
    "        distance[u] = {}\n",
    "        for v in G.nodes:\n",
    "            distance[u][v] = float('inf')\n",
    "    for u in nodes:\n",
    "        distance[u][u] = 0\n",
    "\n",
    "    edges = list(G.edges(data=True))\n",
    "    for u, v, data in edges:\n",
    "        weight = data.get('weight', 1)\n",
    "        distance[u][v] = weight\n",
    "\n",
    "    for k in nodes:\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                if distance[i][k] != float('inf') and distance[k][j] != float('inf'):\n",
    "                    if distance[i][j] > distance[i][k] + distance[k][j]:\n",
    "                        distance[i][j] = distance[i][k] + distance[k][j]\n",
    "\n",
    "    for node in nodes:\n",
    "        if distance[node][node] < 0:\n",
    "            pass\n",
    "\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    distance = floyd_warshall(G)\n",
    "    for source, targets in distance.items():\n",
    "        print(f\"Distances with {source} source:\", targets)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our vs built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [10, 25, 50, 75, 100]\n",
    "edge_prob = 0.5\n",
    "\n",
    "testing_algorithms(sizes, edge_prob, nx.floyd_warshall_predecessor_and_distance, floyd_warshall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excecution Times:\n",
    "For smaller graph sizes like 10 vertices, both our implementation and the built-in Floyd-Warshall have very similar execution times, with a difference of less than 1 second.\n",
    "### As the graph size increases:\n",
    "\n",
    "* For 25 vertices, our implementation starts to take more time than the built-in version (about 2.5 seconds vs 1.5 seconds).\n",
    "* For 50 vertices, the time difference becomes more noticeable. Our implementation is taking around 8 seconds, compared to the built-in version's 5 seconds.\n",
    "* For 100 vertices, the gap is much larger, our implementation takes over 15 seconds, while the built-in algorithm takes about 17.5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful explanations\n",
    "### How to get list of edges for your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(G.edges()) # by default G.edges are EdgesView class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get edges with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(G.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on time measuring\n",
    "\n",
    "Read more on this: https://realpython.com/python-timer/\n",
    "\n",
    "Recall that you should measure times for 5, 10, 20, 50, 100, 200, 500 nodes 1000 times (and take mean of time taken for each node amount).\n",
    "\n",
    "Then you should build the plot for two algorithms (x - data size, y - mean time of execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_ITERATIONS = 1000\n",
    "time_taken = 0\n",
    "for i in tqdm(range(NUM_OF_ITERATIONS)):\n",
    "    \n",
    "    # note that we should not measure time of graph creation\n",
    "    G = gnp_random_connected_graph(100, 0.4, False)\n",
    "    \n",
    "    start = time.time()\n",
    "    tree.minimum_spanning_tree(G, algorithm=\"prim\")\n",
    "    end = time.time()\n",
    "    \n",
    "    time_taken += end - start\n",
    "\n",
    "time_taken / NUM_OF_ITERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn package\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General idea\n",
    "\n",
    "#### You are expected to write a quite simple, yet good core logic of decision tree classifier class. Additionaly, you need to test your results and write down a report on what you've done, which principles used and explain the general process.\n",
    "\n",
    "#### Hopefully, you have already learned what is decision tree classifier and how it work. For better understanding, and in case if something is still unclear for you, here are some useful links on basics of DTC:\n",
    "- https://www.youtube.com/watch?v=ZVR2Way4nwQ\n",
    "- https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e\n",
    "- https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775\n",
    "- https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial\n",
    "- https://towardsdatascience.com/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b6057\n",
    "\n",
    "#### Also, for those interested to learn more about machine learning and particulary Desicion Trees - here is a great course on Coursera (you may be interested in the whole course or just this particular week):\n",
    "- https://www.coursera.org/learn/advanced-learning-algorithms/home/week/4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n",
    "#### You can use Iris dataset for this task. It is a very popular dataset for machine learning and data science. It contains 150 samples of 3 different species of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. \n",
    "Read more on this: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "#### However, using more interesting and intricate datasets is much appreciated. You can use any dataset you want, but it should be a classification one. For example you can use breast cancer or wine datasets, which are also available in sklearn.datasets. Or you can use any other dataset you find interesting.\n",
    "P.S. In case you are not sure if your dataset is suitable, feel free to ask assistants :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "dir(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we have 150 entries (samples, infos about a flower). The columns being: Sepal Length, Sepal Width, Petal Length and Petal Width(features). Let's look at first two entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To undestand data little bit better, let's plot some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Sepal width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can clearly see, that even basing on those two parameters, we can clearly divide (classify) out data into several groups. For this, we will use decision tree classifier: https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
    "\n",
    "### Example of usage\n",
    "\n",
    "\n",
    "**Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression**. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split\n",
    "\n",
    "We train our model using training dataset and evaluate its performance basing on the test dataset. Reason to use two separate datasets is that our model learns its parameters from data, thus test set allows us to check its possibilities on completely new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(X, y, test_size= 0.20)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model learning\n",
    "\n",
    "It learns its parameters (where it should split data and for what threshold value) basing on the training dataset. It is done by minimizing some cost function (e.g. Gini impurity or entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of produced tree\n",
    "\n",
    "You do not need to understand this piece of code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=iris.feature_names,  \n",
    "                     class_names=iris.target_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction step\n",
    "\n",
    "Now we can use our model to predict which type has a flower, basing on its parameters.\n",
    "\n",
    "This is conducted basically via traversing the tree that you can see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also measure the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predictions == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get clearer intuition about predicion, let's look at those X, that should be labeled to some flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here you can traverse the tree above by yourself and make sure that prediction works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([X_test[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, it is your turn to write such classifier by yourself!\n",
    "\n",
    "####  Gini impurity\n",
    "\n",
    "Decision trees use the concept of Gini impurity to describe how “pure” a node is. A node is pure (G = 0) if all its samples belong to the same class, while a node with many samples from many different classes will have a Gini closer to 1.\n",
    "\n",
    "$G = 1 - \\sum_{k=1}^{n}p_{k}^2$\n",
    "\n",
    "For example, if a node contains five samples, with two belonging to the first class (first flower), two of class 2, one of class 3 and none of class 4, then\n",
    "\n",
    "$G = 1 - (\\frac{2}{5})^2 - (\\frac{2}{5})^2 - (\\frac{1}{5})^2 = 0.64$\n",
    "\n",
    "#### Remarks \n",
    "- We recommend using additional functions in `DecisionTreeClassifier` class, to make the implementation process easier.\n",
    "- [use this hint](https://arc.net/l/quote/pqvyjqei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, X: npt.NDArray, y: npt.NDArray):\n",
    "        \"\"\"\n",
    "        :param X: numpy array of form [[feature1,feature2, ... featureN], ...] (i.e. [[1.5, 5.4, 3.2, 9.8] , ...] for case with iris d.s.)\n",
    "        :param y: numpy array of from [class1, class2, ...] (i.e. [0,1,1,2,1,0,...] for case with iris d.s.)\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth: int) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.number_of_classes = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X: npt.NDArray, y: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Basically, function that performs all the training (building of a tree)\n",
    "        We recommend to use it as a wrapper of recursive building function\n",
    "        \"\"\"\n",
    "        self.number_of_classes = np.unique(y).size\n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self, X_test: npt.NDArray) -> list:\n",
    "        \"\"\"\n",
    "        Traverse the tree while there is a child\n",
    "        and return the predicted class for it \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(X_test: list[list], y_test: list) -> float:\n",
    "    \"\"\"\n",
    "    Returns accuracy of the model (ratio of right guesses to the number of samples)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***put your code below, with usage and evaluation of Decision classifier tree*** (delete this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
